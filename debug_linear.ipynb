{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from main_ce import set_loader\n",
    "from util import AverageMeter\n",
    "from util import adjust_learning_rate, warmup_learning_rate, accuracy\n",
    "from util import set_optimizer\n",
    "# from networks.resnet_big import SupConResNet, LinearClassifier\n",
    "from networks.EN import SupConEN, LinearClassifier\n",
    "\n",
    "try:\n",
    "    import apex\n",
    "    from apex import amp, optimizers\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(opt):\n",
    "#     model = SupConResNet(name=opt.model)\n",
    "    model = SupConEN(name=opt.model)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)\n",
    "\n",
    "    ckpt = torch.load(opt.ckpt, map_location='cpu')\n",
    "    state_dict = ckpt['model']\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model.encoder = torch.nn.DataParallel(model.encoder)\n",
    "        else:\n",
    "            new_state_dict = {}\n",
    "            for k, v in state_dict.items():\n",
    "                k = k.replace(\"module.\", \"\")\n",
    "                new_state_dict[k] = v\n",
    "            state_dict = new_state_dict\n",
    "        model = model.cuda()\n",
    "        classifier = classifier.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    return model, classifier, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):\n",
    "    \"\"\"one epoch training\"\"\"\n",
    "    model.eval()\n",
    "    classifier.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "        bsz = labels.shape[0]\n",
    "\n",
    "        # warm-up learning rate\n",
    "        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n",
    "\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            features = model.encoder(images)\n",
    "        output = classifier(features.detach())\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # update metric\n",
    "        losses.update(loss.item(), bsz)\n",
    "        acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n",
    "        top1.update(acc1[0], bsz)\n",
    "\n",
    "        # SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # print info\n",
    "        if (idx + 1) % opt.print_freq == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'loss {loss.val:.3f} ({loss.avg:.3f})\\t'\n",
    "                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return losses.avg, top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, classifier, criterion, opt):\n",
    "    \"\"\"validation\"\"\"\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (images, labels) in enumerate(val_loader):\n",
    "            images = images.float().cuda()\n",
    "            labels = labels.cuda()\n",
    "            bsz = labels.shape[0]\n",
    "\n",
    "            # forward\n",
    "            output = classifier(model.encoder(images))\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # update metric\n",
    "            losses.update(loss.item(), bsz)\n",
    "            acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n",
    "            top1.update(acc1[0], bsz)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if idx % opt.print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                       idx, len(val_loader), batch_time=batch_time,\n",
    "                       loss=losses, top1=top1))\n",
    "\n",
    "    print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "    return losses.avg, top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--ckpt'], dest='ckpt', nargs=None, const=None, default='', type=<class 'str'>, choices=None, help='path to pre-trained model', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc = 0\n",
    "parser = argparse.ArgumentParser('argument for training')\n",
    "\n",
    "parser.add_argument('--print_freq', type=int, default=10,\n",
    "                    help='print frequency')\n",
    "parser.add_argument('--save_freq', type=int, default=50,\n",
    "                    help='save frequency')\n",
    "parser.add_argument('--batch_size', type=int, default=256,\n",
    "                    help='batch_size')\n",
    "parser.add_argument('--num_workers', type=int, default=16,\n",
    "                    help='num of workers to use')\n",
    "parser.add_argument('--epochs', type=int, default=100,\n",
    "                    help='number of training epochs')\n",
    "# optimization\n",
    "parser.add_argument('--learning_rate', type=float, default=0.1,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',\n",
    "                    help='where to decay lr, can be a list')\n",
    "parser.add_argument('--lr_decay_rate', type=float, default=0.2,\n",
    "                    help='decay rate for learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                    help='momentum')\n",
    "\n",
    "# model dataset\n",
    "parser.add_argument('--model', type=str, default='resnet50')\n",
    "parser.add_argument('--dataset', type=str, default='cifar10',\n",
    "                    choices=['cifar10', 'cifar100', 'path'], help='dataset')\n",
    "parser.add_argument('--mean', type=str, help='mean of dataset in path in form of str tuple')\n",
    "parser.add_argument('--std', type=str, help='std of dataset in path in form of str tuple')\n",
    "parser.add_argument('--data_folder', type=str, default=None, help='path to custom dataset')\n",
    "parser.add_argument('--n_cls', type=int, default=2)\n",
    "\n",
    "\n",
    "# other setting\n",
    "parser.add_argument('--cosine', action='store_true',\n",
    "                    help='using cosine annealing')\n",
    "parser.add_argument('--warm', action='store_true',\n",
    "                    help='warm-up for large batch training')\n",
    "\n",
    "parser.add_argument('--ckpt', type=str, default='',\n",
    "                    help='path to pre-trained model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = parser.parse_args([\"--batch_size=2\", \"--learning_rate=5\", \"--dataset=path\", \"--data_folder=./datasets\", \"--mean=(0.4914, 0.4822, 0.4465)\", \\\n",
    "                   \"--std=(0.2675, 0.2565, 0.2761)\", \\\n",
    "                         \"--ckpt=last.pth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eval(opt.mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = opt.lr_decay_epochs.split(',')\n",
    "opt.lr_decay_epochs = list([])\n",
    "for it in iterations:\n",
    "    opt.lr_decay_epochs.append(int(it))\n",
    "    \n",
    "# set the path according to the environment\n",
    "opt.data_folder = './datasets/'\n",
    "\n",
    "opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\\\n",
    "    format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,\n",
    "           opt.batch_size)\n",
    "\n",
    "if opt.cosine:\n",
    "    opt.model_name = '{}_cosine'.format(opt.model_name)\n",
    "\n",
    "# warm-up for large-batch training,\n",
    "if opt.warm:\n",
    "    opt.model_name = '{}_warm'.format(opt.model_name)\n",
    "    opt.warmup_from = 0.01\n",
    "    opt.warm_epochs = 10\n",
    "    if opt.cosine:\n",
    "        eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "        opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n",
    "                1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
    "    else:\n",
    "        opt.warmup_to = opt.learning_rate\n",
    "\n",
    "if opt.dataset == 'cifar10':\n",
    "    opt.n_cls = 10\n",
    "elif opt.dataset == 'cifar100':\n",
    "    opt.n_cls = 100\n",
    "elif opt.dataset == 'path':\n",
    "    opt.n_cls = 2\n",
    "else:\n",
    "    raise ValueError('dataset not supported: {}'.format(opt.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data loader\n",
    "train_loader, val_loader = set_loader(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model and criterion\n",
    "model, classifier, criterion = set_model(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(1, 3, 240, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0102,  0.0023]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.forward(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build optimizer\n",
    "optimizer = set_optimizer(opt, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 10\n",
       "    Root location: ./datasets/\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.4914, 0.4822, 0.4465))\n",
       "           )"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3a582dfcd402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtime1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     loss, acc = train(train_loader, model, classifier, criterion,\n\u001b[0m\u001b[1;32m      8\u001b[0m                       optimizer, epoch, opt)\n\u001b[1;32m      9\u001b[0m     \u001b[0mtime2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9c32e21d4ff2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, classifier, criterion, optimizer, epoch, opt)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbsz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ct/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# training routine\n",
    "for epoch in range(1, opt.epochs + 1):\n",
    "    adjust_learning_rate(opt, optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    time1 = time.time()\n",
    "    loss, acc = train(train_loader, model, classifier, criterion,\n",
    "                      optimizer, epoch, opt)\n",
    "    time2 = time.time()\n",
    "    print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(\n",
    "        epoch, time2 - time1, acc))\n",
    "\n",
    "    # eval for one epoch\n",
    "    loss, val_acc = validate(val_loader, model, classifier, criterion, opt)\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "\n",
    "print('best accuracy: {:.2f}'.format(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)\n",
    "images, labels = it.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(80) > np.asarray('60,75,90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adjust_learning_rate(args, optimizer, epoch):\n",
    "# parser.add_argument('--learning_rate', type=float, default=0.1,\n",
    "#                     help='learning rate')\n",
    "# parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',\n",
    "#                     help='where to decay lr, can be a list')\n",
    "# parser.add_argument('--lr_decay_rate', type=float, default=0.2,\n",
    "#                     help='decay rate for learning rate')\n",
    "# parser.add_argument('--weight_decay', type=float, default=0,\n",
    "#                     help='weight decay')\n",
    "# parser.add_argument('--momentum', type=float, default=0.9,\n",
    "#                     help='momentum')\n",
    "\n",
    "lr = 0.1\n",
    "epoch = 1\n",
    "# if args.cosine:\n",
    "#     eta_min = lr * (args.lr_decay_rate ** 3)\n",
    "#     lr = eta_min + (lr - eta_min) * (\n",
    "#             1 + math.cos(math.pi * epoch / args.epochs)) / 2\n",
    "# else:\n",
    "steps = np.sum(epoch > np.asarray('60,75,90'))\n",
    "if steps > 0:\n",
    "    lr = lr * (0.2 ** steps)\n",
    "\n",
    "# for param_group in optimizer.param_groups:\n",
    "#     param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decay_epochs='60,75,90'\n",
    "iterations = lr_decay_epochs.split(',')\n",
    "lr_decay_epochs = list([])\n",
    "for it in iterations:\n",
    "    lr_decay_epochs.append(int(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(lr_decay_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(epoch > np.asarray(lr_decay_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
